{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Jay Urbain, PhD\n",
    "\n",
    "References:\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning, with applications in R, www.StatLearning.com, Springer-Verlag, New York. Chapter 4.\n",
    "\n",
    "- [scikit-learn](http://scikit-learn.org/stable/) \n",
    "\n",
    "*Advantages of decision trees are:*\n",
    "\n",
    "- Simple to understand and to interpret. Trees can be visualised.\n",
    "\n",
    "- Can fit complex non-linear decision spaces.\n",
    "\n",
    "- Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note: some decision tree models require support for missing values.\n",
    "\n",
    "- The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.\n",
    "\n",
    "- Able to handle both numerical and categorical data. Other techniques are usually specialized in analysing datasets that have only one type of variable. \n",
    "\n",
    "- Able to handle multi-output problems.\n",
    "\n",
    "- Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained. By contrast, in a black box model (e.g., in a neural network), results may be more difficult to interpret.\n",
    "\n",
    "*The disadvantages of decision trees include:*\n",
    "\n",
    "- Decision-tree learners can create over-complex trees that do not generalize the data well. This is called **overfitting**. Mechanisms such as pruning (not currently supported by scikit-learn), setting the minimum number of samples required at a leaf node, or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble, i.e., combining multiple decision trees.\n",
    "\n",
    "- The problem of learning an optimal decision tree is known to be *NP-complete* under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the **greedy algorithm**, where locally optimal decisions are made at each node. \n",
    "\n",
    "- Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\n",
    "\n",
    "- There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "- Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Nonparametric statistics are statistics *not* based on parameterized families of probability distributions. For example, assuming we have a gaussian distribution that can be *parameterized* by the mean and variance of the distribution. The non-parametric model does not have any parameters: parameters are determined by the training data, not the model.\n",
    "\n",
    "For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. \n",
    "\n",
    "The deeper the tree, the more complex the decision rules and the *fitter* the model.\n",
    "\n",
    "Below, a Random Forest is used to create an ensemble model of multiple trees. The decision trees are used to fit a sine curve with an addition of a noisy observation. As a result, it learns local linear regressions approximating the sine curve.\n",
    "\n",
    "We can see that if the maximum depth of the tree (controlled by the max_depth parameter) is set too high, the decision trees learn too fine details of the training data and learn from the noise, i.e. they overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEZCAYAAABmTgnDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104cf0d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots within notebook versus launching a separate window\n",
    "%matplotlib inline \n",
    "\n",
    "# Import the necessary modules and libraries\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "regr_2 = DecisionTreeRegressor(max_depth=5)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"k\", label=\"data\")\n",
    "plt.plot(X_test, y_1, c=\"g\", label=\"max_depth=2\", linewidth=2)\n",
    "plt.plot(X_test, y_2, c=\"r\", label=\"max_depth=5\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can build complex decision boundaries by dividing the feature\n",
    "space into rectangles. However, we have to be careful since the deeper the decision tree, the more complex the decision boundary becomes, which can easily result in overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression trees\n",
    "\n",
    "Goal: **predict a baseball player's Salary** based on **Years** (number of years playing in the major leagues) and **Hits** (number of hits he made in the previous year). Here is the training data, represented visually (low salary is blue/green, high salary is red/yellow):\n",
    "\n",
    "<img src=\"salary_color.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How might you \"stratify\" or \"segment\" the feature space into regions, based on salary?** Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "\n",
    "Below is a regression tree that has been fit to the data by a computer. (We will talk later about how the fitting algorithm actually works.) Note that  Salary is measured in thousands and has been log-transformed.\n",
    "\n",
    "<img src=\"salary_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we make Salary predictions (for out-of-sample data) using a decision tree?**\n",
    "\n",
    "- Start at the top, and examine the first \"splitting rule\" (Years < 4.5).\n",
    "- If the rule is True for a given player, follow the left branch. If the rule is False, follow the right branch.\n",
    "- Continue until reaching the bottom. The predicted Salary is the number in that particular \"bucket\".\n",
    "- *Side note:* Years and Hits are both integers, but the convention is to label these rules using the midpoint between adjacent values.\n",
    "\n",
    "Examples predictions:\n",
    "\n",
    "- Years=3, then predict 5.11 ($\\$1000 \\times e^{5.11} \\approx \\$166000$)\n",
    "- Years=5 and Hits=100, then predict 6.00 ($\\$1000 \\times e^{6.00} \\approx \\$403000$)\n",
    "- Years=8 and Hits=120, then predict 6.74 ($\\$1000 \\times e^{6.74} \\approx \\$846000$)\n",
    "\n",
    "**How did we come up with the numbers at the bottom of the tree?** Each number is just the **mean Salary in the training data** of players who fit that criteria. Here's the same diagram as before, split into the three regions:\n",
    "\n",
    "<img src=\"salary_regions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Using the decision tree above, what would you predict for the log salary of a player who has played for 4 years and has 150 hits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Imagine that you are doing cost complexity pruning as discussed in class. You fit two trees, $T_1$ is fit at $\\alpha=1$, and $T_2$ is fit at $\\alpha=2$\n",
    "\n",
    "Which of the following is true?\n",
    "\n",
    "- $T_1$ will have at least as many nodes as $T_2$\n",
    "- $T_1$ will have at most as many nodes as  $T_2$\n",
    "- Not enough information is given in the problem to decide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "You have a bag of marbles with 64 red marbles and 36 blue marbles.\n",
    "What is the value of the Gini Index for that bag? Give your answer to the nearest hundredth:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What is the value of the Cross-Entropy? Give your answer to the nearest hundredth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree classification with the used vehicle data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors   type\n",
       "0   22000  2012   13000      2    car\n",
       "1   14000  2010   30000      2    car\n",
       "2   13000  2010   73500      4    car\n",
       "3    9500  2009   78000      4    car\n",
       "4    9000  2007   47000      4    car\n",
       "5    4000  2006  124000      2    car\n",
       "6    3000  2004  177000      4    car\n",
       "7    2000  2004  209000      4  truck\n",
       "8    3000  2003  138000      2    car\n",
       "9    1900  2003  160000      4    car\n",
       "10   2500  2003  190000      2  truck\n",
       "11   5000  2001   62000      4    car\n",
       "12   1800  1999  163000      2  truck\n",
       "13   1300  1997  138000      4    car"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# read in vehicle data\n",
    "vehicles = pd.read_csv('used_vehicles.csv')\n",
    "\n",
    "# print out data\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert car to 0 and truck to 1\n",
    "vehicles['type'] = vehicles.type.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select feature columns (every column except for the 0th column)\n",
    "feature_cols = vehicles.columns[1:]\n",
    "\n",
    "# define X (features) and y (response)\n",
    "X = vehicles[feature_cols]\n",
    "y = vehicles.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year   miles  doors  type\n",
      "10  2003  190000      2     1\n",
      "4   2007   47000      4     0\n",
      "1   2010   30000      2     0\n",
      "12  1999  163000      2     1\n",
      "0   2012   13000      2     0\n",
      "13  1997  138000      4     0\n",
      "9   2003  160000      4     0\n",
      "8   2003  138000      2     0\n",
      "11  2001   62000      4     0\n",
      "5   2006  124000      2     0\n",
      "10     2500\n",
      "4      9000\n",
      "1     14000\n",
      "12     1800\n",
      "0     22000\n",
      "13     1300\n",
      "9      1900\n",
      "8      3000\n",
      "11     5000\n",
      "5      4000\n",
      "Name: price, dtype: int64\n",
      "   year   miles  doors  type\n",
      "3  2009   78000      4     0\n",
      "7  2004  209000      4     1\n",
      "6  2004  177000      4     0\n",
      "2  2010   73500      4     0\n",
      "3     9500\n",
      "7     2000\n",
      "6     3000\n",
      "2    13000\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print out each of the arrays\n",
    "print X_train\n",
    "print y_train\n",
    "print X_test\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import class, instantiate estimator, fit with training set\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5000.  1900.  1900.  5000.]\n",
      "3     9500\n",
      "7     2000\n",
      "6     3000\n",
      "2    13000\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "preds = treereg.predict(X_test)\n",
    "\n",
    "# print predictions and actual values\n",
    "print preds\n",
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4622.4993239588475"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use cross-validation to find best max_depth\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Evaluate decision tree depths of 2, 3, 4, and 5. Identify the decision tree depth that provides the lowest mean square error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4704.0052694797387"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: try max_depth=4\n",
    "treereg = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=4, max_features=None,\n",
       "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=4, fit a tree using your optimal tree depth parameter with ALL DATA - it is not necessarily 4\n",
    "treereg = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3    type    0.000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the \"Gini importance\" of each feature: the (normalized) total reduction of MSE brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: u'vehicles.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-3ea2b419b437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vehicles.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/IPython/core/display.pyc\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: u'vehicles.png'"
     ]
    }
   ],
   "source": [
    "# create a Graphviz file\n",
    "from sklearn.tree import export_graphviz\n",
    "with open(\"vehicles.dot\", 'wb') as f:\n",
    "    f = export_graphviz(treereg, out_file=f, feature_names=feature_cols)\n",
    "\n",
    "# at the command line, run this to convert to PNG:\n",
    "# dot -Tpng 15_vehicles.dot -o vehicles.png\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename='vehicles.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a tree diagram\n",
    "\n",
    "How do we read this decision tree?\n",
    "\n",
    "**Internal nodes:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node before splitting\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- first line is the condition used to split that node (go left if true, go right if false)\n",
    "\n",
    "**Leaves:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node\n",
    "- \"value\" is the mean response value in that node\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against \"value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting for out-of-sample data\n",
    "\n",
    "How accurate is scikit-learn's regression tree at predicting the out-of-sample data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is the regression tree model at predicting out-of-sample data? I.e., data that it has not been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  type\n",
       "0   3000  2003  130000      4     1\n",
       "1   6000  2005   82500      4     0\n",
       "2  12000  2010   60000      2     0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in out-of-sample data\n",
    "oos = pd.read_csv('used_vehicles_oos.csv')\n",
    "\n",
    "# convert car to 0 and truck to 1\n",
    "oos['type'] = oos.type.map({'car':0, 'truck':1})\n",
    "\n",
    "# print data\n",
    "oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_oos = oos[feature_cols]\n",
    "y_oos = oos.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4000.   5000.  13500.]\n",
      "[ 3000  6000 12000]\n"
     ]
    }
   ],
   "source": [
    "# make predictions on out-of-sample data\n",
    "preds = treereg.predict(X_oos)\n",
    "\n",
    "# print predictions and actual values\n",
    "print preds\n",
    "print y_oos.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_oos, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE for the tree you created\n",
    "your_preds = [4000, 5000, 13500]\n",
    "np.sqrt(metrics.mean_squared_error(y_oos, your_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification trees\n",
    "\n",
    "Here is a comparison between classification trees and regression trees:\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize a measure of heterogeneity|\n",
    "\n",
    "Note that classification trees handle **more than two response classes**! \n",
    "\n",
    "Here's an **example of a classification tree**, which predicts whether or not a patient who presented with chest pain has heart disease:\n",
    "\n",
    "<img src=\"heart_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting criteria for classification trees\n",
    "\n",
    "Here are common options for the splitting criteria:\n",
    "\n",
    "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "- **Gini index:** measure of total variance across classes in a region\n",
    "- **cross-entropy:** numerically similar to Gini index, but uses logarithms\n",
    "\n",
    "Which to use?\n",
    "\n",
    "- When growing a tree, Gini index and cross-entropy are better measures of \"node purity\" than classification error rate. The Gini index is faster to compute than cross-entropy, so it is generally preferred (and is used by scikit-learn by default).\n",
    "- When pruning a tree, classification error rate is preferable in order to maximize predictive accuracy.\n",
    "\n",
    "Why do some splits result in leaves with the same predicted class?\n",
    "\n",
    "- The split was performed to increase node purity, even though it didn't reduce the classification error.\n",
    "- Node purity is important because we're interested in the class proportions among the observations in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical predictors\n",
    "\n",
    "Some implementations of classification trees will allow you to handle categorical predictors **without creating dummy variables**. When splitting on a categorical predictor, they will try splitting on **every possible combination of categories** to find the best split. In the example above, \"ChestPain:bc\" means that the left-hand branch consists of observations with the second and third ChestPain categories, and the right-hand branch consists of remaining observations.\n",
    "\n",
    "**Unfortunately, scikit-learn's classification tree implementation does not support this approach.** Instead, here's how you can handle categorical predictors:\n",
    "\n",
    "- If a predictor only has **two possible values**, code it as a single binary variable (0 or 1). Since it's treated as a number, splits will naturally occur at 0.5.\n",
    "- If a predictor has **three or more possible values that are ordered**, code it as a single variable (1, 2, 3, etc). Splits will naturally occur at 1.5, 2.5, etc.\n",
    "- If a predictor has **three or more possible values that are unordered**, create dummy variables and drop one level as usual. The decision tree won't know that the dummy variables are related to one another, but that shouldn't matter in terms of predictive accuracy.\n",
    "- If a predictor has **thousands of possible unordered values**, then it may be best to code it as a single variable (1, 2, 3, etc) instead of using dummy variables to minimize the size of the resulting model. ([reference](http://stackoverflow.com/a/18736132/1636598))\n",
    "\n",
    "We'll see examples of these strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a classification tree in scikit-learn\n",
    "\n",
    "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2         1       3                             Heikkinen, Miss. Laina   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4         0       3                           Allen, Mr. William Henry   \n",
       "5         0       3                                   Moran, Mr. James   \n",
       "6         0       1                            McCarthy, Mr. Timothy J   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)   \n",
       "\n",
       "      sex  age  sibsp  parch            ticket     fare cabin embarked  \n",
       "0    male   22      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female   38      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female   26      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female   35      1      0            113803  53.1000  C123        S  \n",
       "4    male   35      0      0            373450   8.0500   NaN        S  \n",
       "5    male  NaN      0      0            330877   8.4583   NaN        Q  \n",
       "6    male   54      0      0             17463  51.8625   E46        S  \n",
       "7    male    2      3      1            349909  21.0750   NaN        S  \n",
       "8  female   27      0      2            347742  11.1333   NaN        S  \n",
       "9  female   14      1      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived      0\n",
       "pclass        0\n",
       "name          0\n",
       "sex           0\n",
       "age         177\n",
       "sibsp         0\n",
       "parch         0\n",
       "ticket        0\n",
       "fare          0\n",
       "cabin       687\n",
       "embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for missing values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose our response and a few features, and decide whether we need to adjust them:\n",
    "\n",
    "- **survived:** This is our response, and is already encoded as 0=died and 1=survived.\n",
    "- **pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). They are ordered, so we'll leave them as-is.\n",
    "- **sex:** This is a binary category, so we should encode as 0=female and 1=male.\n",
    "- **age:** We need to fill in the missing values.\n",
    "- **embarked:** This is the port they emarked from. There are three unordered categories, so we'll create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  sex  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris    1   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
       "2         1       3                             Heikkinen, Miss. Laina    0   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
       "4         0       3                           Allen, Mr. William Henry    1   \n",
       "5         0       3                                   Moran, Mr. James    1   \n",
       "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
       "\n",
       "         age  sibsp  parch            ticket     fare cabin embarked  \n",
       "0  22.000000      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  38.000000      1      0          PC 17599  71.2833   C85        C  \n",
       "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  35.000000      1      0            113803  53.1000  C123        S  \n",
       "4  35.000000      0      0            373450   8.0500   NaN        S  \n",
       "5  29.699118      0      0            330877   8.4583   NaN        Q  \n",
       "6  54.000000      0      0             17463  51.8625   E46        S  \n",
       "7   2.000000      3      1            349909  21.0750   NaN        S  \n",
       "8  27.000000      0      2            347742  11.1333   NaN        S  \n",
       "9  14.000000      1      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode sex feature\n",
    "titanic['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in missing values for age\n",
    "titanic.age.fillna(titanic.age.mean(), inplace=True)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   embarked_C  embarked_Q  embarked_S\n",
       "0           0           0           1\n",
       "1           1           0           0\n",
       "2           0           0           1\n",
       "3           0           0           1\n",
       "4           0           0           1\n",
       "5           0           1           0\n",
       "6           0           0           1\n",
       "7           0           0           1\n",
       "8           0           0           1\n",
       "9           1           0           0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three dummy variables using get_dummies\n",
    "pd.get_dummies(titanic.embarked, prefix='embarked').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  sex  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris    1   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
       "2         1       3                             Heikkinen, Miss. Laina    0   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
       "4         0       3                           Allen, Mr. William Henry    1   \n",
       "5         0       3                                   Moran, Mr. James    1   \n",
       "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
       "\n",
       "         age  sibsp  parch            ticket     fare cabin embarked  \\\n",
       "0  22.000000      1      0         A/5 21171   7.2500   NaN        S   \n",
       "1  38.000000      1      0          PC 17599  71.2833   C85        C   \n",
       "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
       "3  35.000000      1      0            113803  53.1000  C123        S   \n",
       "4  35.000000      0      0            373450   8.0500   NaN        S   \n",
       "5  29.699118      0      0            330877   8.4583   NaN        Q   \n",
       "6  54.000000      0      0             17463  51.8625   E46        S   \n",
       "7   2.000000      3      1            349909  21.0750   NaN        S   \n",
       "8  27.000000      0      2            347742  11.1333   NaN        S   \n",
       "9  14.000000      1      0            237736  30.0708   NaN        C   \n",
       "\n",
       "   embarked_Q  embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  \n",
       "5           1           0  \n",
       "6           0           1  \n",
       "7           0           1  \n",
       "8           0           1  \n",
       "9           0           0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.embarked, prefix='embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "# note: axis=0 means rows, axis=1 means columns\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of feature columns\n",
    "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "# define X and y\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "with open(\"15_titanic.dot\", 'wb') as f:\n",
    "    f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the split in the bottom right, which was made only to increase node purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pclass</td>\n",
       "      <td>0.242664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.655584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>0.064494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embarked_Q</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embarked_S</td>\n",
       "      <td>0.037258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      pclass    0.242664\n",
       "1         sex    0.655584\n",
       "2         age    0.064494\n",
       "3  embarked_Q    0.000000\n",
       "4  embarked_S    0.037258"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Create a titanic decision tree using cross-entropy instead of the gini index. \n",
    "\n",
    "Compute feature importance and compare the results between cross-entropy and the gini index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Create a titanic decision tree using the top 3 features. Calculate the accuracy of your tree. Compare the results to a tree using all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees summary\n",
    "\n",
    "Here are some advantages and disadvantages of decision trees that we haven't yet talked about:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
    "- Non-parametric (will do better than linear regression if relationship between predictors and response is highly non-linear)\n",
    "\n",
    "\n",
    "**Figure:**\n",
    "\n",
    "- **Left: linear model decision boundary fit (e.g., simple logistic regression)**\n",
    "- **Right: non-linear decision boundary fit (decision tree)**\n",
    "\n",
    "<img src=\"linear_vs_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "- Small variations in the data can result in a completely different tree\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "- Can create biased trees if the classes are highly imbalanced\n",
    "\n",
    "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
